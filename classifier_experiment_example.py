"""
åˆ†ç±»å™¨å®éªŒç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ•°æ®é›†åˆ’åˆ†è¿›è¡ŒResNet18åˆ†ç±»å®éªŒ
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, ConcatDataset, Dataset
from torchvision.models import resnet18
from tqdm import tqdm
import json
from pathlib import Path
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import seaborn as sns
import random

from load_dataset import MicroDopplerDataset


def set_random_seed(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # ç¡®ä¿CUDAæ“ä½œçš„ç¡®å®šæ€§
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    print(f"ğŸ² å·²è®¾ç½®éšæœºç§å­: {seed} (ç»“æœå¯é‡ç°)")


def train_classifier(model, train_loader, criterion, optimizer, device, epochs=15, scheduler=None):
    """è®­ç»ƒåˆ†ç±»å™¨ - å›ºå®šepochæ•°ï¼Œè®­ç»ƒå®Œæˆåå†æµ‹è¯•"""
    
    print(f"è®­ç»ƒæ•°æ®ä¿¡æ¯ï¼š{len(train_loader.dataset)} å¼ å›¾åƒ")
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ªbatchçš„å›¾åƒå°ºå¯¸
    for images, labels in train_loader:
        print(f"å›¾åƒå°ºå¯¸: {images.shape} (Batch, Channels, Height, Width)")
        print(f"å›¾åƒæ•°æ®ç±»å‹: {images.dtype}, å€¼åŸŸ: [{images.min():.3f}, {images.max():.3f}]")
        break
    
    print(f"å¼€å§‹è®­ç»ƒ {epochs} epochsï¼ˆä¸æ–‡çŒ®ä¸€è‡´ï¼‰")
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            pbar.set_postfix({
                'loss': f'{total_loss/(pbar.n+1):.4f}',
                'train_acc': f'{100.*correct/total:.2f}%'
            })
        
        avg_train_loss = total_loss / len(train_loader)
        train_acc = 100. * correct / total
        
        print(f"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Train Acc = {train_acc:.2f}%")
        
        # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆåŸºäºè®­ç»ƒlossï¼‰
        if scheduler:
            scheduler.step(avg_train_loss)
    
    print(f"è®­ç»ƒå®Œæˆï¼Œå…±è¿›è¡Œ {epochs} epochsï¼ˆä¸æ–‡çŒ®ä¸€è‡´ï¼‰")


def extract_features(model, data_loader, device, max_samples=1000):
    """æå–ç‰¹å¾ç”¨äºt-SNEå¯è§†åŒ–"""
    model.eval()
    
    # åˆ›å»ºç‰¹å¾æå–å™¨ï¼ˆå»æ‰æœ€åçš„åˆ†ç±»å±‚ï¼‰
    feature_extractor = nn.Sequential(*list(model.children())[:-1])
    
    features = []
    labels = []
    
    sample_count = 0
    with torch.no_grad():
        for images, batch_labels in tqdm(data_loader, desc="Extracting features"):
            if sample_count >= max_samples:
                break
                
            images = images.to(device)
            
            # æå–ç‰¹å¾
            batch_features = feature_extractor(images)  # [batch, 512, 1, 1]
            batch_features = batch_features.view(batch_features.size(0), -1)  # [batch, 512]
            
            features.append(batch_features.cpu().numpy())
            labels.extend(batch_labels.numpy())
            
            sample_count += len(batch_labels)
    
    features = np.concatenate(features, axis=0)[:max_samples]
    labels = np.array(labels)[:max_samples]
    
    return features, labels


def visualize_tsne_comparison(model, test_loader, device, per_class_accuracy):
    """å¯¹æ¯”å¯è§†åŒ–ï¼šå‡†ç¡®ç‡æœ€é«˜vsæœ€ä½çš„ç”¨æˆ·ï¼Œç”Ÿæˆä¸¤å¼ ç‹¬ç«‹å›¾ç‰‡"""
    
    # æ ¹æ®å‡†ç¡®ç‡æ’åºç”¨æˆ·
    user_accuracies = [(i, acc) for i, acc in enumerate(per_class_accuracy)]
    user_accuracies.sort(key=lambda x: x[1], reverse=True)
    
    # é€‰æ‹©æœ€é«˜å’Œæœ€ä½å‡†ç¡®ç‡çš„ç”¨æˆ·
    top_5_users = [user_id for user_id, _ in user_accuracies[:5]]
    bottom_5_users = [user_id for user_id, _ in user_accuracies[-5:]]
    
    print(f"å‡†ç¡®ç‡æœ€é«˜çš„5ä¸ªç”¨æˆ·: {top_5_users}")
    print(f"å¯¹åº”å‡†ç¡®ç‡: {[user_accuracies[i][1] for i in range(5)]}")
    print(f"å‡†ç¡®ç‡æœ€ä½çš„5ä¸ªç”¨æˆ·: {bottom_5_users}")  
    print(f"å¯¹åº”å‡†ç¡®ç‡: {[user_accuracies[i][1] for i in range(-5, 0)]}")
    
    # æ·±è‰²é¢œè‰²æ˜ å°„ï¼šçº¢ã€é»„ã€è“ã€ç»¿ã€ç´«
    colors = ['#CC0000', '#B8860B', '#000080', '#006400', '#4B0082']  # æ·±çº¢ã€æ·±é»„ã€æ·±è“ã€æ·±ç»¿ã€æ·±ç´«
    
    # === ç¬¬ä¸€å¼ å›¾ï¼šé«˜å‡†ç¡®ç‡ç”¨æˆ· ===
    features_high, labels_high = extract_features_for_users(
        model, test_loader, device, target_users=top_5_users, max_per_user=50
    )
    
    print("å¼€å§‹t-SNEé™ç»´ï¼ˆé«˜å‡†ç¡®ç‡ç”¨æˆ·ï¼‰...")
    tsne_high = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)
    features_2d_high = tsne_high.fit_transform(features_high)
    
    # ç»˜åˆ¶é«˜å‡†ç¡®ç‡ç”¨æˆ·
    plt.figure(figsize=(10, 8))
    for i, user_id in enumerate(top_5_users):
        mask = labels_high == user_id
        if mask.sum() > 0:
            plt.scatter(features_2d_high[mask, 0], features_2d_high[mask, 1], 
                       c=colors[i], label=f'User {user_id} ({user_accuracies[i][1]:.1f}%)', 
                       alpha=0.8, s=40, edgecolors='black', linewidth=0.8)
    
    plt.title('High Accuracy Users (Top 5)\nTest Set Feature Distribution', fontsize=16, fontweight='bold')
    plt.xlabel('t-SNE Component 1', fontsize=12)
    plt.ylabel('t-SNE Component 2', fontsize=12)
    plt.legend(fontsize=12, frameon=True, fancybox=True, shadow=True)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('tsne_high_accuracy_users.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # === ç¬¬äºŒå¼ å›¾ï¼šä½å‡†ç¡®ç‡ç”¨æˆ· ===
    features_low, labels_low = extract_features_for_users(
        model, test_loader, device, target_users=bottom_5_users, max_per_user=50
    )
    
    print("å¼€å§‹t-SNEé™ç»´ï¼ˆä½å‡†ç¡®ç‡ç”¨æˆ·ï¼‰...")
    tsne_low = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)
    features_2d_low = tsne_low.fit_transform(features_low)
    
    # ç»˜åˆ¶ä½å‡†ç¡®ç‡ç”¨æˆ·
    plt.figure(figsize=(10, 8))
    for i, user_id in enumerate(bottom_5_users):
        mask = labels_low == user_id
        if mask.sum() > 0:
            acc_idx = len(user_accuracies) - 5 + i  # è®¡ç®—åœ¨æ’åºåˆ—è¡¨ä¸­çš„æ­£ç¡®ç´¢å¼•
            plt.scatter(features_2d_low[mask, 0], features_2d_low[mask, 1], 
                       c=colors[i], label=f'User {user_id} ({user_accuracies[acc_idx][1]:.1f}%)', 
                       alpha=0.8, s=40, edgecolors='black', linewidth=0.8)
    
    plt.title('Low Accuracy Users (Bottom 5)\nTest Set Feature Distribution', fontsize=16, fontweight='bold')
    plt.xlabel('t-SNE Component 1', fontsize=12)
    plt.ylabel('t-SNE Component 2', fontsize=12)
    plt.legend(fontsize=12, frameon=True, fancybox=True, shadow=True)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('tsne_low_accuracy_users.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("é«˜å‡†ç¡®ç‡ç”¨æˆ·t-SNEå·²ä¿å­˜è‡³: tsne_high_accuracy_users.png")
    print("ä½å‡†ç¡®ç‡ç”¨æˆ·t-SNEå·²ä¿å­˜è‡³: tsne_low_accuracy_users.png")


def extract_features_for_users(model, data_loader, device, target_users=None, max_per_user=50):
    """ä¸ºç‰¹å®šç”¨æˆ·æå–ç‰¹å¾"""
    model.eval()
    
    # åˆ›å»ºç‰¹å¾æå–å™¨
    feature_extractor = nn.Sequential(*list(model.children())[:-1])
    
    features = []
    labels = []
    user_counts = {user_id: 0 for user_id in target_users} if target_users else {}
    
    with torch.no_grad():
        for images, batch_labels in tqdm(data_loader, desc="Extracting specific user features"):
            images = images.to(device)
            
            for i, label in enumerate(batch_labels):
                user_id = label.item()
                
                # åªå¤„ç†ç›®æ ‡ç”¨æˆ·
                if target_users and user_id not in target_users:
                    continue
                    
                # æ§åˆ¶æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬æ•°
                if target_users and user_counts[user_id] >= max_per_user:
                    continue
                
                # æå–å•å¼ å›¾åƒçš„ç‰¹å¾
                single_img = images[i:i+1]
                feature = feature_extractor(single_img)  # [1, 512, 1, 1]
                feature = feature.view(-1)  # [512]
                
                features.append(feature.cpu().numpy())
                labels.append(user_id)
                
                if target_users:
                    user_counts[user_id] += 1
    
    features = np.stack(features)
    labels = np.array(labels)
    
    print(f"æå–äº† {len(features)} ä¸ªç‰¹å¾ï¼Œæ¶‰åŠç”¨æˆ·: {np.unique(labels)}")
    return features, labels


def evaluate_classifier(model, test_loader, device, num_classes, visualize=True):
    """è¯„ä¼°åˆ†ç±»å™¨ï¼ŒåŒ…å«è¿‡æ‹Ÿåˆæ£€æŸ¥å’Œå¯è§†åŒ–"""
    model.eval()
    
    correct = 0
    total = 0
    per_class_correct = [0] * num_classes
    per_class_total = [0] * num_classes
    
    # æ”¶é›†é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ
    all_confidences = []
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc="Evaluating"):
            images, labels = images.to(device), labels.to(device)
            
            outputs = model(images)
            probabilities = torch.softmax(outputs, dim=1)
            confidences, predicted = probabilities.max(1)
            
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # æ”¶é›†æ•°æ®ç”¨äºåˆ†æ
            all_confidences.extend(confidences.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«
            for label, pred in zip(labels, predicted):
                per_class_total[label.item()] += 1
                if label == pred:
                    per_class_correct[label.item()] += 1
    
    overall_acc = 100. * correct / total
    
    print(f"\næ•´ä½“å‡†ç¡®ç‡: {overall_acc:.2f}% ({correct}/{total})")
    
    # åˆ†æç½®ä¿¡åº¦åˆ†å¸ƒï¼ˆè¿‡æ‹Ÿåˆæ£€æŸ¥ï¼‰
    confidences = np.array(all_confidences)
    print(f"\nç½®ä¿¡åº¦åˆ†æï¼ˆè¿‡æ‹Ÿåˆæ£€æŸ¥ï¼‰:")
    print(f"  å¹³å‡ç½®ä¿¡åº¦: {confidences.mean():.3f}")
    print(f"  ç½®ä¿¡åº¦æ ‡å‡†å·®: {confidences.std():.3f}")
    print(f"  é«˜ç½®ä¿¡åº¦æ ·æœ¬æ¯”ä¾‹ (>0.9): {(confidences > 0.9).mean():.3f}")
    print(f"  ä½ç½®ä¿¡åº¦æ ·æœ¬æ¯”ä¾‹ (<0.5): {(confidences < 0.5).mean():.3f}")
    
    # å¦‚æœç½®ä¿¡åº¦è¿‡äºé›†ä¸­åœ¨é«˜å€¼ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ
    if confidences.mean() > 0.95:
        print("  âš ï¸  è­¦å‘Š: å¹³å‡ç½®ä¿¡åº¦è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆï¼")
    if (confidences > 0.99).mean() > 0.5:
        print("  âš ï¸  è­¦å‘Š: è¶…è¿‡ä¸€åŠæ ·æœ¬ç½®ä¿¡åº¦>0.99ï¼Œå¼ºçƒˆæ€€ç–‘è¿‡æ‹Ÿåˆï¼")
    
    # æ¯ä¸ªç”¨æˆ·çš„å‡†ç¡®ç‡
    print("\nå„ç”¨æˆ·å‡†ç¡®ç‡:")
    for i in range(num_classes):
        if per_class_total[i] > 0:
            acc = 100. * per_class_correct[i] / per_class_total[i]
            print(f"  ç”¨æˆ·{i:2d}: {acc:5.2f}% ({per_class_correct[i]}/{per_class_total[i]})")
    
    # è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„å‡†ç¡®ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
    per_class_accuracy = []
    for i in range(num_classes):
        if per_class_total[i] > 0:
            acc = 100. * per_class_correct[i] / per_class_total[i]
            per_class_accuracy.append(acc)
        else:
            per_class_accuracy.append(0.0)
    
    # t-SNEå¯¹æ¯”å¯è§†åŒ–
    if visualize:
        print("\nè¿›è¡Œt-SNEå¯¹æ¯”å¯è§†åŒ–ï¼ˆåŸºäºæµ‹è¯•é›†ï¼‰...")
        visualize_tsne_comparison(model, test_loader, device, per_class_accuracy)
    
    return overall_acc


class SyntheticDataset(Dataset):
    """
    åŠ è½½ç”Ÿæˆçš„åˆæˆå›¾åƒ
    æ”¯æŒæ ¼å¼ï¼šID_X/sample_XXX.png æˆ– ID_X/generated_XXX.jpg
    """
    def __init__(self, synthetic_folder, transform=None):
        self.samples = []
        
        synthetic_path = Path(synthetic_folder)
        
        # æœç´¢å­æ–‡ä»¶å¤¹ä¸­çš„å›¾åƒï¼šID_X/*.png æˆ– ID_X/*.jpg
        for user_folder in sorted(synthetic_path.glob("ID_*")):
            if user_folder.is_dir():
                # ä»æ–‡ä»¶å¤¹åè§£æç”¨æˆ·IDï¼šID_1 â†’ label=0, ID_2 â†’ label=1
                user_id = int(user_folder.name.split('_')[1])  # ID_1 â†’ 1
                label = user_id - 1  # ID_1 â†’ label=0
                
                # åŠ è½½è¯¥ç”¨æˆ·æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰å›¾åƒï¼ˆæ”¯æŒpngå’Œjpgï¼‰
                img_files = list(user_folder.glob("*.png")) + list(user_folder.glob("*.jpg"))
                for img_path in sorted(img_files):
                    self.samples.append((img_path, label))
        
        self.transform = transform
        print(f"âœ“ åŠ è½½åˆæˆæ•°æ®é›†: {len(self.samples)}å¼ å›¾åƒï¼Œ{len(set(l for _, l in self.samples))}ä¸ªç”¨æˆ·")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        img = Image.open(img_path).convert('RGB')
        
        if self.transform:
            img = self.transform(img)
        
        return img, label


def main():
    """
    å®Œæ•´çš„åˆ†ç±»å™¨å®éªŒæµç¨‹
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='ResNet18åˆ†ç±»å™¨å®éªŒ')
    parser.add_argument('--data_root', type=str, required=True,
                        help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--split_file', type=str,
                        default='./latents_cache/data_split.json',
                        help='æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶')
    parser.add_argument('--synthetic_folder', type=str, default=None,
                        help='åˆæˆæ•°æ®æ–‡ä»¶å¤¹ï¼ˆå¯é€‰ï¼Œç”¨äºå¢å¼ºå®éªŒï¼‰')
    parser.add_argument('--num_users', type=int, default=None,
                        help='ä½¿ç”¨çš„ç”¨æˆ·æ•°é‡ï¼ˆå¦‚æœæä¾›åˆæˆæ•°æ®ï¼Œè‡ªåŠ¨åŒ¹é…åˆæˆæ•°æ®çš„ç”¨æˆ·æ•°ï¼‰')
    parser.add_argument('--batch_size', type=int, default=64,
                        help='è®­ç»ƒbatch sizeï¼ˆé»˜è®¤64ï¼Œé€‚ä¸­çš„baselineä¾¿äºçªå‡ºåˆæˆæ•°æ®çš„æå‡ï¼‰')
    parser.add_argument('--epochs', type=int, default=15)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--seed', type=int, default=42,
                        help='éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°')
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­ - åœ¨æ‰€æœ‰æ“ä½œä¹‹å‰
    set_random_seed(args.seed)
    
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    
    # åŠ è½½æ•°æ®é›†
    print("="*60)
    print("åŠ è½½æ•°æ®é›†")
    print("="*60)
    
    # è®­ç»ƒé›†ï¼ˆçœŸå®å›¾åƒï¼‰
    train_ds = MicroDopplerDataset(
        data_root=args.data_root,
        split_file=args.split_file,
        split='train',
        use_latents=False
    )
    
    # æµ‹è¯•é›†ï¼ˆçœŸå®å›¾åƒï¼‰
    test_ds = MicroDopplerDataset(
        data_root=args.data_root,
        split_file=args.split_file,
        split='test',
        use_latents=False
    )
    
    # å¦‚æœæä¾›äº†åˆæˆæ•°æ®ï¼Œè‡ªåŠ¨æ£€æµ‹ç”¨æˆ·æ•°å¹¶è¿‡æ»¤çœŸå®æ•°æ®
    if args.synthetic_folder:
        print("\næ·»åŠ åˆæˆæ•°æ®åˆ°è®­ç»ƒé›†...")
        
        # ä¸çœŸå®å›¾åƒä½¿ç”¨ç›¸åŒçš„transform
        synthetic_ds = SyntheticDataset(
            args.synthetic_folder,
            transform=train_ds.transform
        )
        
        # è‡ªåŠ¨æ£€æµ‹åˆæˆæ•°æ®çš„ç”¨æˆ·æ•°
        synthetic_users = set(label for _, label in synthetic_ds.samples)
        num_synthetic_users = len(synthetic_users)
        max_label = max(synthetic_users)
        
        print(f"æ£€æµ‹åˆ°åˆæˆæ•°æ®åŒ…å« {num_synthetic_users} ä¸ªç”¨æˆ·ï¼ˆlabel 0-{max_label}ï¼‰")
        
        # å¦‚æœæœªæŒ‡å®šnum_usersï¼Œè‡ªåŠ¨ä½¿ç”¨åˆæˆæ•°æ®çš„ç”¨æˆ·æ•°
        if args.num_users is None:
            args.num_users = max_label + 1  # labelä»0å¼€å§‹ï¼Œæ‰€ä»¥+1
            print(f"è‡ªåŠ¨è®¾ç½®ä¸ºä½¿ç”¨å‰ {args.num_users} ä¸ªç”¨æˆ·")
        
        # è¿‡æ»¤çœŸå®æ•°æ®ï¼Œåªä¿ç•™å‰num_usersä¸ªç”¨æˆ·
        print(f"è¿‡æ»¤çœŸå®æ•°æ®ï¼Œåªä¿ç•™å‰ {args.num_users} ä¸ªç”¨æˆ·ï¼ˆlabel 0-{args.num_users-1}ï¼‰...")
        train_ds.samples = [(path, label) for path, label in train_ds.samples if label < args.num_users]
        test_ds.samples = [(path, label) for path, label in test_ds.samples if label < args.num_users]
        
        print(f"è¿‡æ»¤åè®­ç»ƒé›†: {len(train_ds)} å¼ çœŸå®å›¾åƒ")
        print(f"è¿‡æ»¤åæµ‹è¯•é›†: {len(test_ds)} å¼ çœŸå®å›¾åƒ")
        
        # åˆå¹¶æ•°æ®é›†
        train_ds = ConcatDataset([train_ds, synthetic_ds])
        print(f"å¢å¼ºåè®­ç»ƒé›†: {len(train_ds)} å¼ ï¼ˆçœŸå®+åˆæˆï¼‰")
    elif args.num_users is not None:
        # æ²¡æœ‰åˆæˆæ•°æ®ï¼Œä½†æŒ‡å®šäº†num_usersï¼Œä¹Ÿè¿‡æ»¤
        print(f"\nè¿‡æ»¤æ•°æ®ï¼Œåªä½¿ç”¨å‰ {args.num_users} ä¸ªç”¨æˆ·ï¼ˆlabel 0-{args.num_users-1}ï¼‰...")
        train_ds.samples = [(path, label) for path, label in train_ds.samples if label < args.num_users]
        test_ds.samples = [(path, label) for path, label in test_ds.samples if label < args.num_users]
        print(f"è¿‡æ»¤åè®­ç»ƒé›†: {len(train_ds)} å¼ å›¾åƒ")
        print(f"è¿‡æ»¤åæµ‹è¯•é›†: {len(test_ds)} å¼ å›¾åƒ")
    
    # è‡ªåŠ¨æ¨æ–­ç±»åˆ«æ•°é‡ï¼ˆä»æ•°æ®é›†ä¸­è·å–ï¼‰
    if isinstance(train_ds, ConcatDataset):
        # ConcatDatasetåŒ…å«å¤šä¸ªå­æ•°æ®é›†ï¼Œä»ç¬¬ä¸€ä¸ªï¼ˆçœŸå®æ•°æ®é›†ï¼‰è·å–
        all_labels = [label for _, label in train_ds.datasets[0].samples]
    else:
        all_labels = [label for _, label in train_ds.samples]
    num_classes = len(set(all_labels))
    print(f"\næ£€æµ‹åˆ° {num_classes} ä¸ªç”¨æˆ·ç±»åˆ«")
    
    # åˆ›å»ºDataLoaderï¼ˆWindowsç³»ç»Ÿä½¿ç”¨num_workers=0é¿å…å¤šè¿›ç¨‹é—®é¢˜ï¼‰
    train_loader = DataLoader(
        train_ds, batch_size=args.batch_size,
        shuffle=True, num_workers=0, pin_memory=True
    )
    
    test_loader = DataLoader(
        test_ds, batch_size=args.batch_size,
        shuffle=False, num_workers=0, pin_memory=True
    )
    
    # åˆ›å»ºResNet18åˆ†ç±»å™¨
    print(f"\nåˆ›å»ºResNet18åˆ†ç±»å™¨ï¼ˆä¸ä½¿ç”¨é¢„è®­ç»ƒï¼Œ{num_classes}ä¸ªç±»åˆ«ï¼‰...")
    model = resnet18(weights=None, num_classes=num_classes)
    model = model.to(device)
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)  # ç§»é™¤weight_decay
    
    # ä¸ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆé¿å…æ­£åˆ™åŒ–æ•ˆæœï¼‰
    scheduler = None
    
    # è®­ç»ƒ
    print("\nå¼€å§‹è®­ç»ƒ...")
    train_classifier(
        model, train_loader, criterion, optimizer, device, args.epochs, scheduler
    )
    
    # è®­ç»ƒå®Œæˆåè¿›è¡Œæµ‹è¯•ï¼ˆåŒ…å«t-SNEå¯è§†åŒ–ï¼‰
    print("\nè¯„ä¼°åˆ†ç±»å™¨...")
    accuracy = evaluate_classifier(model, test_loader, device, num_classes, visualize=True)
    
    # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
    model_save_dir = Path("./trained_models")
    model_save_dir.mkdir(parents=True, exist_ok=True)
    
    # æ ¹æ®æ˜¯å¦ä½¿ç”¨åˆæˆæ•°æ®ç”Ÿæˆä¸åŒçš„æ–‡ä»¶å
    if args.synthetic_folder:
        model_name = f"classifier_real_synthetic_acc{accuracy:.2f}_seed{args.seed}.pth"
        print(f"\nğŸ’¾ ä¿å­˜å¢å¼ºæ¨¡å‹: {model_name}")
    else:
        model_name = f"classifier_real_only_acc{accuracy:.2f}_seed{args.seed}.pth"
        print(f"\nğŸ’¾ ä¿å­˜åŸºçº¿æ¨¡å‹: {model_name}")
    
    model_save_path = model_save_dir / model_name
    
    # ä¿å­˜å®Œæ•´çš„checkpointä¿¡æ¯
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'accuracy': accuracy,
        'args': vars(args),
        'epoch': args.epochs,
        'model_info': {
            'architecture': 'ResNet18',
            'num_classes': num_classes,
            'input_size': (256, 256, 3),
            'data_type': 'real+synthetic' if args.synthetic_folder else 'real_only'
        }
    }
    
    torch.save(checkpoint, model_save_path)
    print(f"   æ¨¡å‹å·²ä¿å­˜è‡³: {model_save_path}")
    print(f"   æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.2f}%")
    if args.synthetic_folder:
        print(f"   æ•°æ®ç±»å‹: çœŸå®å›¾åƒ + åˆæˆå›¾åƒ")
    else:
        print(f"   æ•°æ®ç±»å‹: ä»…çœŸå®å›¾åƒ")
    
    print(f"\nğŸ† è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {accuracy:.2f}%")
    
    return accuracy


if __name__ == '__main__':
    accuracy = main()

