# åˆ†ç±»å™¨å®éªŒå®Œæ•´æµç¨‹

## ğŸ“‹ å®éªŒç›®æ ‡

è¯„ä¼°DDPMç”Ÿæˆçš„åˆæˆæ•°æ®å¯¹åˆ†ç±»å™¨æ€§èƒ½çš„æå‡æ•ˆæœã€‚

**å¯¹æ¯”å®éªŒ**ï¼š
- åŸºå‡†åˆ†ç±»å™¨ï¼šä»…ä½¿ç”¨çœŸå®è®­ç»ƒé›†ï¼ˆ31Ã—50=1550å¼ ï¼‰
- å¢å¼ºåˆ†ç±»å™¨ï¼šçœŸå®è®­ç»ƒé›† + åˆæˆæ•°æ®ï¼ˆ1550+Nå¼ ï¼‰
- æµ‹è¯•é›†ï¼šçœŸå®æµ‹è¯•é›†ï¼ˆ31Ã—100=3100å¼ ï¼‰

---

## ğŸ”§ æ•°æ®é›†åˆ’åˆ†ä¿¡æ¯

### data_split.json ç»“æ„

```json
{
  "seed": 42,
  "num_users": 31,
  "images_per_user_train": 50,
  "users": {
    "ID_1": {
      "user_id": 1,
      "label": 0,
      "total_images": 150,
      "train_images": [
        "ID_1/img_001.jpg",
        "ID_1/img_023.jpg",
        ...  // 50å¼ è®­ç»ƒå›¾åƒè·¯å¾„
      ],
      "test_images": [
        "ID_1/img_045.jpg",
        ...  // 100å¼ æµ‹è¯•å›¾åƒè·¯å¾„
      ]
    },
    "ID_2": { ... },
    ...
    "ID_31": { ... }
  }
}
```

### ä½¿ç”¨æ–¹å¼

```python
import json

# åŠ è½½åˆ’åˆ†ä¿¡æ¯
with open('latents_cache/data_split.json', 'r') as f:
    split_info = json.load(f)

# è·å–ç”¨æˆ·1çš„è®­ç»ƒå›¾åƒ
user1_train = split_info['users']['ID_1']['train_images']
# ['ID_1/img_001.jpg', 'ID_1/img_023.jpg', ...]

# è·å–ç”¨æˆ·1çš„æµ‹è¯•å›¾åƒ  
user1_test = split_info['users']['ID_1']['test_images']
# ['ID_1/img_045.jpg', ...]
```

---

## ğŸš€ å®Œæ•´å®éªŒæµç¨‹

### æ­¥éª¤1ï¼šé¢„ç¼–ç æ‰€æœ‰å›¾åƒ

```bash
# ç¼–ç è®­ç»ƒé›†+æµ‹è¯•é›†ï¼ˆ~30åˆ†é’Ÿï¼‰
!python preprocess_latents.py \
    --vae_path /kaggle/input/kl-vae-best-pt/kl_vae_best.pt \
    --data_path /kaggle/input/organized-gait-dataset/Normal_line \
    --encode_all  # ç¼–ç æ‰€æœ‰å›¾åƒï¼ˆåŒ…æ‹¬æµ‹è¯•é›†ï¼‰

# è¾“å‡º:
# Dataset split:
#   Train: 1550 images
#   Test: 3100 images (å‡è®¾æ¯ç”¨æˆ·150å¼ )
#   Total to encode: 4650 images
#
# é¢„ç¼–ç å®Œæˆï¼
# æ–°ç¼–ç : 4650 å¼ 
# 
# è¾“å‡ºæ–‡ä»¶:
#   æ½œåœ¨è¡¨ç¤ºç¼“å­˜: ./latents_cache/
#   æ•°æ®é›†åˆ’åˆ†: ./latents_cache/data_split.json
```

### æ­¥éª¤2ï¼šæŸ¥çœ‹æ•°æ®é›†åˆ’åˆ†

```bash
# æŸ¥çœ‹åˆ’åˆ†æ‘˜è¦
!python load_dataset.py --split_file latents_cache/data_split.json

# è¾“å‡º:
# ============================================================
# æ•°æ®é›†åˆ’åˆ†æ‘˜è¦
# ============================================================
# éšæœºç§å­: 42
# ç”¨æˆ·æ•°é‡: 31
# æ¯ç”¨æˆ·è®­ç»ƒé›†: 50 å¼ 
#
# ç”¨æˆ·       æ ‡ç­¾   æ€»æ•°   è®­ç»ƒ   æµ‹è¯•  
# ------------------------------------------------------------
# ID_1       0      150    50     100   
# ID_2       1      150    50     100   
# ...
# ID_31      30     150    50     100   
# ------------------------------------------------------------
# æ€»è®¡              4650   1550   3100  
```

### æ­¥éª¤3ï¼šè®­ç»ƒDDPM

```bash
# è®­ç»ƒDDPMï¼ˆä½¿ç”¨1550å¼ è®­ç»ƒé›†ï¼‰
!python train_latent_cfg.py

# DDPMåªä½¿ç”¨è®­ç»ƒé›†ï¼Œæµ‹è¯•é›†å¯¹DDPMå®Œå…¨ä¸å¯è§
```

### æ­¥éª¤4ï¼šç”Ÿæˆåˆæˆæ•°æ®

```bash
# ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆ50å¼ åˆæˆå›¾åƒ
!python generate.py \
    --checkpoint results/model-75.pt \
    --all_users \
    --samples_per_user 50 \
    --output_dir synthetic_data/checkpoint_75

# è¾“å‡º: synthetic_data/checkpoint_75/user_XX_sample_YYY.png
```

### æ­¥éª¤5ï¼šåŸºå‡†åˆ†ç±»å™¨å®éªŒ

```bash
# å®éªŒ1: ä»…ä½¿ç”¨çœŸå®è®­ç»ƒé›†
!python classifier_experiment_example.py \
    --data_root /kaggle/input/organized-gait-dataset/Normal_line \
    --split_file latents_cache/data_split.json \
    --epochs 100

# è¾“å‡º:
# æ•´ä½“å‡†ç¡®ç‡: XX.XX% (baseline)
```

### æ­¥éª¤6ï¼šå¢å¼ºåˆ†ç±»å™¨å®éªŒ

```bash
# å®éªŒ2: çœŸå®è®­ç»ƒé›† + åˆæˆæ•°æ®
!python classifier_experiment_example.py \
    --data_root /kaggle/input/organized-gait-dataset/Normal_line \
    --split_file latents_cache/data_split.json \
    --synthetic_folder synthetic_data/checkpoint_75 \
    --epochs 100

# è¾“å‡º:
# å¢å¼ºåè®­ç»ƒé›†: 3100 å¼ ï¼ˆçœŸå®+åˆæˆï¼‰
# æ•´ä½“å‡†ç¡®ç‡: YY.YY% (enhanced)
#
# æå‡: (YY.YY - XX.XX)%
```

---

## ğŸ“Š å®Œæ•´å¯¹æ¯”å®éªŒ

### Pythonè„šæœ¬ç¤ºä¾‹

```python
"""
å®Œæ•´çš„å¯¹æ¯”å®éªŒæµç¨‹
è¯„ä¼°å¤šä¸ªDDPMæ£€æŸ¥ç‚¹
"""

from load_dataset import MicroDopplerDataset
from classifier_experiment_example import train_classifier, evaluate_classifier
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, ConcatDataset
from torchvision.models import resnet18

# é…ç½®
data_root = '/kaggle/input/organized-gait-dataset/Normal_line'
split_file = './latents_cache/data_split.json'
checkpoints = [50, 75, 100, 125, 150]  # milestoneç¼–å·
device = 'cuda'

# åŠ è½½çœŸå®æ•°æ®é›†
print("åŠ è½½çœŸå®æ•°æ®é›†...")
real_train_ds = MicroDopplerDataset(
    data_root=data_root,
    split_file=split_file,
    split='train'
)

test_ds = MicroDopplerDataset(
    data_root=data_root,
    split_file=split_file,
    split='test'
)

test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)

# === å®éªŒ1: åŸºå‡†åˆ†ç±»å™¨ ===
print("\n" + "="*60)
print("å®éªŒ1: åŸºå‡†åˆ†ç±»å™¨ï¼ˆä»…çœŸå®æ•°æ®ï¼‰")
print("="*60)

model_baseline = resnet18(pretrained=False, num_classes=31).to(device)
optimizer = torch.optim.Adam(model_baseline.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

train_loader = DataLoader(real_train_ds, batch_size=32, shuffle=True)
train_classifier(model_baseline, train_loader, criterion, optimizer, device, epochs=100)

acc_baseline = evaluate_classifier(model_baseline, test_loader, device)
print(f"\nåŸºå‡†å‡†ç¡®ç‡: {acc_baseline:.2f}%")

# === å®éªŒ2: å¯¹æ¯ä¸ªæ£€æŸ¥ç‚¹è¯„ä¼° ===
results = {}

for milestone in checkpoints:
    print("\n" + "="*60)
    print(f"å®éªŒ2.{milestone}: å¢å¼ºåˆ†ç±»å™¨ï¼ˆæ£€æŸ¥ç‚¹{milestone}ï¼‰")
    print("="*60)
    
    # åŠ è½½åˆæˆæ•°æ®
    synthetic_folder = f'synthetic_data/checkpoint_{milestone}'
    
    from torchvision import transforms
    from PIL import Image
    
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(256),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])
    
    synthetic_ds = SyntheticDataset(synthetic_folder, transform=transform)
    
    # åˆå¹¶æ•°æ®é›†
    mixed_train_ds = ConcatDataset([real_train_ds, synthetic_ds])
    mixed_train_loader = DataLoader(mixed_train_ds, batch_size=32, shuffle=True)
    
    # è®­ç»ƒå¢å¼ºåˆ†ç±»å™¨
    model_enhanced = resnet18(pretrained=False, num_classes=31).to(device)
    optimizer = torch.optim.Adam(model_enhanced.parameters(), lr=1e-3)
    
    train_classifier(model_enhanced, mixed_train_loader, criterion, optimizer, device, epochs=100)
    
    # è¯„ä¼°
    acc_enhanced = evaluate_classifier(model_enhanced, test_loader, device)
    
    improvement = acc_enhanced - acc_baseline
    results[milestone] = {
        'accuracy': acc_enhanced,
        'improvement': improvement
    }
    
    print(f"\næ£€æŸ¥ç‚¹{milestone}å‡†ç¡®ç‡: {acc_enhanced:.2f}%")
    print(f"æå‡: {improvement:+.2f}%")

# === æ€»ç»“ ===
print("\n" + "="*60)
print("å®éªŒæ€»ç»“")
print("="*60)
print(f"åŸºå‡†å‡†ç¡®ç‡ï¼ˆä»…çœŸå®ï¼‰: {acc_baseline:.2f}%")
print("\nå„æ£€æŸ¥ç‚¹ç»“æœ:")

for milestone in checkpoints:
    r = results[milestone]
    print(f"  Checkpoint {milestone:3d}: {r['accuracy']:5.2f}% ({r['improvement']:+5.2f}%)")

# æ‰¾åˆ°æœ€ä½³æ£€æŸ¥ç‚¹
best_milestone = max(results, key=lambda k: results[k]['accuracy'])
print(f"\næœ€ä½³æ£€æŸ¥ç‚¹: {best_milestone}")
print(f"æœ€ä½³å‡†ç¡®ç‡: {results[best_milestone]['accuracy']:.2f}%")
print(f"æœ€å¤§æå‡: {results[best_milestone]['improvement']:+.2f}%")


if __name__ == '__main__':
    main()

