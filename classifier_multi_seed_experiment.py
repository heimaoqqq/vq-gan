"""
å¤šç§å­åˆ†ç±»å™¨å®éªŒ
ä¾æ¬¡è®­ç»ƒç§å­ä¸º 6, 42, 888 çš„åˆ†ç±»å™¨
ç»Ÿè®¡æ¯ä¸ªç§å­çš„æµ‹è¯•å‡†ç¡®ç‡ï¼Œè®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, ConcatDataset
from torchvision.models import resnet18
from tqdm import tqdm
import json
from pathlib import Path
from PIL import Image
import numpy as np
import random
import argparse
import logging
from typing import List, Dict

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

from load_dataset import MicroDopplerDataset


def set_random_seed(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # ç¡®ä¿CUDAæ“ä½œçš„ç¡®å®šæ€§
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    print(f"ğŸ² å·²è®¾ç½®éšæœºç§å­: {seed}")


def train_classifier(model, train_loader, criterion, optimizer, device, epochs=15, scheduler=None):
    """è®­ç»ƒåˆ†ç±»å™¨ - å›ºå®šepochæ•°ï¼Œè®­ç»ƒå®Œæˆåå†æµ‹è¯•"""
    
    print(f"è®­ç»ƒæ•°æ®ä¿¡æ¯ï¼š{len(train_loader.dataset)} å¼ å›¾åƒ")
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ªbatchçš„å›¾åƒå°ºå¯¸
    for images, labels in train_loader:
        print(f"å›¾åƒå°ºå¯¸: {images.shape} (Batch, Channels, Height, Width)")
        print(f"å›¾åƒæ•°æ®ç±»å‹: {images.dtype}, å€¼åŸŸ: [{images.min():.3f}, {images.max():.3f}]")
        break
    
    print(f"å¼€å§‹è®­ç»ƒ {epochs} epochsï¼ˆä¸æ–‡çŒ®ä¸€è‡´ï¼‰")
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            pbar.set_postfix({
                'loss': f'{total_loss/(pbar.n+1):.4f}',
                'train_acc': f'{100.*correct/total:.2f}%'
            })
        
        avg_train_loss = total_loss / len(train_loader)
        train_acc = 100. * correct / total
        
        print(f"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Train Acc = {train_acc:.2f}%")
        
        # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆåŸºäºè®­ç»ƒlossï¼‰
        if scheduler:
            scheduler.step(avg_train_loss)
    
    print(f"è®­ç»ƒå®Œæˆï¼Œå…±è¿›è¡Œ {epochs} epochsï¼ˆä¸æ–‡çŒ®ä¸€è‡´ï¼‰")


def evaluate_classifier(model, test_loader, device, num_classes):
    """è¯„ä¼°åˆ†ç±»å™¨"""
    model.eval()
    
    correct = 0
    total = 0
    per_class_correct = [0] * num_classes
    per_class_total = [0] * num_classes
    
    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc="Evaluating"):
            images, labels = images.to(device), labels.to(device)
            
            outputs = model(images)
            _, predicted = outputs.max(1)
            
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«
            for label, pred in zip(labels, predicted):
                per_class_total[label.item()] += 1
                if label == pred:
                    per_class_correct[label.item()] += 1
    
    overall_acc = 100. * correct / total
    
    print(f"\næ•´ä½“å‡†ç¡®ç‡: {overall_acc:.2f}% ({correct}/{total})")
    
    # æ¯ä¸ªç”¨æˆ·çš„å‡†ç¡®ç‡
    print("\nå„ç”¨æˆ·å‡†ç¡®ç‡:")
    for i in range(num_classes):
        if per_class_total[i] > 0:
            acc = 100. * per_class_correct[i] / per_class_total[i]
            print(f"  ç”¨æˆ·{i:2d}: {acc:5.2f}% ({per_class_correct[i]}/{per_class_total[i]})")
    
    return overall_acc, per_class_correct, per_class_total


class SyntheticDataset(torch.utils.data.Dataset):
    """
    åŠ è½½ç”Ÿæˆçš„åˆæˆå›¾åƒ
    æ”¯æŒæ ¼å¼ï¼šID_X/sample_XXX.png æˆ– ID_X/generated_XXX.jpg
    """
    def __init__(self, synthetic_folder, transform=None):
        self.samples = []
        
        synthetic_path = Path(synthetic_folder)
        
        # æœç´¢å­æ–‡ä»¶å¤¹ä¸­çš„å›¾åƒï¼šID_X/*.png æˆ– ID_X/*.jpg
        for user_folder in sorted(synthetic_path.glob("ID_*")):
            if user_folder.is_dir():
                # ä»æ–‡ä»¶å¤¹åè§£æç”¨æˆ·IDï¼šID_1 â†’ label=0, ID_2 â†’ label=1
                user_id = int(user_folder.name.split('_')[1])  # ID_1 â†’ 1
                label = user_id - 1  # ID_1 â†’ label=0
                
                # åŠ è½½è¯¥ç”¨æˆ·æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰å›¾åƒï¼ˆæ”¯æŒpngå’Œjpgï¼‰
                img_files = list(user_folder.glob("*.png")) + list(user_folder.glob("*.jpg"))
                for img_path in sorted(img_files):
                    self.samples.append((img_path, label))
        
        self.transform = transform
        print(f"âœ“ åŠ è½½åˆæˆæ•°æ®é›†: {len(self.samples)}å¼ å›¾åƒï¼Œ{len(set(l for _, l in self.samples))}ä¸ªç”¨æˆ·")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        img = Image.open(img_path).convert('RGB')
        
        if self.transform:
            img = self.transform(img)
        
        return img, label


def run_single_experiment(seed: int, args, device) -> Dict:
    """è¿è¡Œå•ä¸ªç§å­çš„å®éªŒ"""
    
    logger.info("\n" + "="*60)
    logger.info(f"å¼€å§‹å®éªŒ: Seed = {seed}")
    logger.info("="*60)
    
    # è®¾ç½®éšæœºç§å­
    set_random_seed(seed)
    
    # åŠ è½½æ•°æ®é›†
    logger.info("åŠ è½½æ•°æ®é›†...")
    
    # è®­ç»ƒé›†ï¼ˆçœŸå®å›¾åƒï¼‰
    train_ds = MicroDopplerDataset(
        data_root=args.data_root,
        split_file=args.split_file,
        split='train',
        use_latents=False
    )
    
    # æµ‹è¯•é›†ï¼ˆçœŸå®å›¾åƒï¼‰
    test_ds = MicroDopplerDataset(
        data_root=args.data_root,
        split_file=args.split_file,
        split='test',
        use_latents=False
    )
    
    # å¦‚æœæä¾›äº†åˆæˆæ•°æ®
    if args.synthetic_folder:
        logger.info("æ·»åŠ åˆæˆæ•°æ®åˆ°è®­ç»ƒé›†...")
        
        synthetic_ds = SyntheticDataset(
            args.synthetic_folder,
            transform=train_ds.transform
        )
        
        # è‡ªåŠ¨æ£€æµ‹åˆæˆæ•°æ®çš„ç”¨æˆ·æ•°
        synthetic_users = set(label for _, label in synthetic_ds.samples)
        num_synthetic_users = len(synthetic_users)
        max_label = max(synthetic_users)
        
        logger.info(f"æ£€æµ‹åˆ°åˆæˆæ•°æ®åŒ…å« {num_synthetic_users} ä¸ªç”¨æˆ·ï¼ˆlabel 0-{max_label}ï¼‰")
        
        # å¦‚æœæœªæŒ‡å®šnum_usersï¼Œè‡ªåŠ¨ä½¿ç”¨åˆæˆæ•°æ®çš„ç”¨æˆ·æ•°
        if args.num_users is None:
            args.num_users = max_label + 1  # labelä»0å¼€å§‹ï¼Œæ‰€ä»¥+1
            logger.info(f"è‡ªåŠ¨è®¾ç½®ä¸ºä½¿ç”¨å‰ {args.num_users} ä¸ªç”¨æˆ·")
        
        # è¿‡æ»¤çœŸå®æ•°æ®ï¼Œåªä¿ç•™å‰num_usersä¸ªç”¨æˆ·
        logger.info(f"è¿‡æ»¤çœŸå®æ•°æ®ï¼Œåªä¿ç•™å‰ {args.num_users} ä¸ªç”¨æˆ·ï¼ˆlabel 0-{args.num_users-1}ï¼‰...")
        train_ds.samples = [(path, label) for path, label in train_ds.samples if label < args.num_users]
        test_ds.samples = [(path, label) for path, label in test_ds.samples if label < args.num_users]
        
        logger.info(f"è¿‡æ»¤åè®­ç»ƒé›†: {len(train_ds)} å¼ çœŸå®å›¾åƒ")
        logger.info(f"è¿‡æ»¤åæµ‹è¯•é›†: {len(test_ds)} å¼ çœŸå®å›¾åƒ")
        
        # åˆå¹¶æ•°æ®é›†
        train_ds = ConcatDataset([train_ds, synthetic_ds])
        logger.info(f"å¢å¼ºåè®­ç»ƒé›†: {len(train_ds)} å¼ ï¼ˆçœŸå®+åˆæˆï¼‰")
    elif args.num_users is not None:
        # æ²¡æœ‰åˆæˆæ•°æ®ï¼Œä½†æŒ‡å®šäº†num_usersï¼Œä¹Ÿè¿‡æ»¤
        logger.info(f"\nè¿‡æ»¤æ•°æ®ï¼Œåªä½¿ç”¨å‰ {args.num_users} ä¸ªç”¨æˆ·ï¼ˆlabel 0-{args.num_users-1}ï¼‰...")
        train_ds.samples = [(path, label) for path, label in train_ds.samples if label < args.num_users]
        test_ds.samples = [(path, label) for path, label in test_ds.samples if label < args.num_users]
        logger.info(f"è¿‡æ»¤åè®­ç»ƒé›†: {len(train_ds)} å¼ å›¾åƒ")
        logger.info(f"è¿‡æ»¤åæµ‹è¯•é›†: {len(test_ds)} å¼ å›¾åƒ")
    
    # è‡ªåŠ¨æ¨æ–­ç±»åˆ«æ•°é‡
    if isinstance(train_ds, ConcatDataset):
        all_labels = [label for _, label in train_ds.datasets[0].samples]
    else:
        all_labels = [label for _, label in train_ds.samples]
    num_classes = len(set(all_labels))
    logger.info(f"æ£€æµ‹åˆ° {num_classes} ä¸ªç”¨æˆ·ç±»åˆ«")
    
    # åˆ›å»ºDataLoader
    train_loader = DataLoader(
        train_ds, batch_size=args.batch_size,
        shuffle=True, num_workers=0, pin_memory=True
    )
    
    test_loader = DataLoader(
        test_ds, batch_size=args.batch_size,
        shuffle=False, num_workers=0, pin_memory=True
    )
    
    # åˆ›å»ºResNet18åˆ†ç±»å™¨
    logger.info(f"åˆ›å»ºResNet18åˆ†ç±»å™¨ï¼ˆ{num_classes}ä¸ªç±»åˆ«ï¼‰...")
    model = resnet18(weights=None, num_classes=num_classes)
    model = model.to(device)
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    scheduler = None
    
    # è®­ç»ƒ
    logger.info("å¼€å§‹è®­ç»ƒ...")
    train_classifier(
        model, train_loader, criterion, optimizer, device, args.epochs, scheduler
    )
    
    # è¯„ä¼°
    logger.info("è¯„ä¼°åˆ†ç±»å™¨...")
    accuracy, per_class_correct, per_class_total = evaluate_classifier(
        model, test_loader, device, num_classes
    )
    
    logger.info(f"\nâœ“ Seed {seed} å®Œæˆ")
    logger.info(f"  æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.2f}%")
    
    # æ‰“å°å„ç”¨æˆ·å‡†ç¡®ç‡
    logger.info("  å„ç”¨æˆ·å‡†ç¡®ç‡:")
    for i in range(num_classes):
        if per_class_total[i] > 0:
            acc = 100. * per_class_correct[i] / per_class_total[i]
            logger.info(f"    ç”¨æˆ·{i:2d}: {acc:5.2f}% ({per_class_correct[i]}/{per_class_total[i]})")
    
    return {
        'seed': seed,
        'accuracy': accuracy,
        'per_class_correct': per_class_correct,
        'per_class_total': per_class_total,
        'num_classes': num_classes
    }


def main():
    parser = argparse.ArgumentParser(description='å¤šç§å­ResNet18åˆ†ç±»å™¨å®éªŒ')
    parser.add_argument('--data_root', type=str, required=True,
                        help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--split_file', type=str,
                        default='./latents_cache/data_split.json',
                        help='æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶')
    parser.add_argument('--synthetic_folder', type=str, default=None,
                        help='åˆæˆæ•°æ®æ–‡ä»¶å¤¹ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--num_users', type=int, default=None,
                        help='ä½¿ç”¨çš„ç”¨æˆ·æ•°é‡')
    parser.add_argument('--batch_size', type=int, default=64,
                        help='è®­ç»ƒbatch size')
    parser.add_argument('--epochs', type=int, default=15)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--seeds', type=int, nargs='+', default=[6, 42, 888],
                        help='è¦æµ‹è¯•çš„ç§å­åˆ—è¡¨')
    
    args = parser.parse_args()
    
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    
    logger.info("\n" + "="*60)
    logger.info("å¤šç§å­åˆ†ç±»å™¨å®éªŒ")
    logger.info("="*60)
    logger.info(f"æ•°æ®æ ¹ç›®å½•: {args.data_root}")
    logger.info(f"è¦æµ‹è¯•çš„ç§å­: {args.seeds}")
    logger.info(f"è®­ç»ƒepochs: {args.epochs}")
    logger.info(f"å­¦ä¹ ç‡: {args.lr}")
    logger.info(f"Batch size: {args.batch_size}")
    logger.info("="*60)
    
    # è¿è¡Œå¤šä¸ªç§å­çš„å®éªŒ
    results = []
    for seed in args.seeds:
        result = run_single_experiment(seed, args, device)
        results.append(result)
    
    # ç»Ÿè®¡ç»“æœ
    logger.info("\n" + "="*60)
    logger.info("å®éªŒç»“æœç»Ÿè®¡")
    logger.info("="*60)
    
    accuracies = [r['accuracy'] for r in results]
    
    logger.info("\nå„ç§å­çš„æµ‹è¯•å‡†ç¡®ç‡:")
    for result in results:
        logger.info(f"  Seed {result['seed']:3d}: {result['accuracy']:6.2f}%")
    
    mean_acc = np.mean(accuracies)
    std_acc = np.std(accuracies)
    min_acc = np.min(accuracies)
    max_acc = np.max(accuracies)
    
    logger.info(f"\nç»Ÿè®¡ä¿¡æ¯:")
    logger.info(f"  å¹³å‡å‡†ç¡®ç‡: {mean_acc:.2f}%")
    logger.info(f"  æ ‡å‡†å·®: {std_acc:.2f}%")
    logger.info(f"  æœ€é«˜å‡†ç¡®ç‡: {max_acc:.2f}% (Seed {results[np.argmax(accuracies)]['seed']})")
    logger.info(f"  æœ€ä½å‡†ç¡®ç‡: {min_acc:.2f}% (Seed {results[np.argmin(accuracies)]['seed']})")
    logger.info(f"  å‡†ç¡®ç‡èŒƒå›´: [{min_acc:.2f}%, {max_acc:.2f}%]")
    
    # ä¿å­˜ç»“æœ
    output_dir = Path("./multi_seed_results")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    results_summary = {
        'seeds': args.seeds,
        'individual_results': [
            {
                'seed': r['seed'],
                'accuracy': float(r['accuracy']),
                'num_classes': r['num_classes']
            }
            for r in results
        ],
        'statistics': {
            'mean_accuracy': float(mean_acc),
            'std_accuracy': float(std_acc),
            'min_accuracy': float(min_acc),
            'max_accuracy': float(max_acc),
            'accuracy_range': [float(min_acc), float(max_acc)]
        },
        'config': {
            'epochs': args.epochs,
            'batch_size': args.batch_size,
            'learning_rate': args.lr,
            'data_root': args.data_root,
            'synthetic_folder': args.synthetic_folder,
            'num_users': args.num_users
        }
    }
    
    results_file = output_dir / "multi_seed_results.json"
    with open(results_file, 'w') as f:
        json.dump(results_summary, f, indent=2)
    
    logger.info(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {results_file}")
    logger.info("="*60)
    
    return results_summary


if __name__ == '__main__':
    results = main()
