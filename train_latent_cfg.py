"""
Latent Diffusion Training with Classifier-Free Guidance
========================================================
å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾åƒç”Ÿæˆ - åŸºäºé¢„è®­ç»ƒVAEçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹

æ•°æ®æµï¼š
  JPGå›¾åƒ(256Ã—256Ã—3) â†’ VAEç¼–ç  â†’ æ½œåœ¨è¡¨ç¤º(32Ã—32Ã—4) â†’ DDPMè®­ç»ƒ

æ•°æ®ç‰¹ç‚¹ä¸æŒ‘æˆ˜ï¼š
  âœ“ æ•°æ®è§„æ¨¡ï¼š31ç”¨æˆ· Ã— 50è®­ç»ƒæ ·æœ¬ = 1550å¼ ï¼ˆå°æ•°æ®é›†ï¼‰
  âœ“ ç”¨æˆ·å†…å˜å¼‚ï¼šæå¤§ï¼ˆæ­¥æ€å‘¨æœŸã€è§’åº¦ã€çŠ¶æ€å·®å¼‚æ˜¾è‘—ï¼‰
  âœ“ ç”¨æˆ·é—´å·®å¼‚ï¼šæå°ï¼ˆè‚‰çœ¼éš¾ä»¥åŒºåˆ†ï¼Œéœ€è¦å¼ºåˆ¤åˆ«èƒ½åŠ›ï¼‰
  âœ— ä¼ ç»Ÿæ•°æ®å¢å¼ºæ•ˆæœå·®
  
å…³é”®è®¾è®¡å†³ç­–ï¼š
  1. æ›´å¤§çš„æ¨¡å‹å®¹é‡ï¼ˆdim=96, 4å±‚ï¼‰â†’ æ•æ‰å¾®å¦™å·®å¼‚
  2. æ›´å¼ºçš„æ¡ä»¶ç¼–ç ï¼ˆ96ç»´ Ã— 31ç±»ï¼‰â†’ æœ‰æ•ˆè¡¨è¾¾ç”¨æˆ·ç‰¹å¾
  3. å¹³è¡¡çš„CFGè®¾ç½®ï¼ˆdrop=0.2, scale=3.5ï¼‰â†’ é¿å…æ¨¡å¼å´©å¡Œ
  4. ç¨³å®šçš„è®­ç»ƒç­–ç•¥ï¼ˆä½lr, æ¢¯åº¦ç´¯ç§¯ï¼‰â†’ é˜²æ­¢è¿‡æ‹Ÿåˆ

Kaggleè·¯å¾„ï¼š
  VAE: /kaggle/input/kl-vae-best-pt/kl_vae_best.pt
  æ•°æ®: /kaggle/input/organized-gait-dataset/Normal_line/ID_*/
"""

import os
import sys
from pathlib import Path
import argparse
from tqdm import tqdm
import math

import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
from PIL import Image
import numpy as np

# å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# ç›´æ¥å¯¼å…¥ï¼Œé¿å…è§¦å‘__init__.py
import importlib.util
spec = importlib.util.spec_from_file_location(
    "cfg_module", 
    os.path.join(os.path.dirname(__file__), 
                 "denoising_diffusion_pytorch/classifier_free_guidance.py")
)
cfg_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(cfg_module)
Unet = cfg_module.Unet
GaussianDiffusion = cfg_module.GaussianDiffusion

from vae.kl_vae import KL_VAE
from accelerate import Accelerator
from ema_pytorch import EMA


# ============================================================
# é…ç½®å‚æ•°
# ============================================================
class Config:
    """é›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®å‚æ•°"""
    
    # === è·¯å¾„é…ç½® ===
    vae_path = '/kaggle/input/kl-vae-best-pt/kl_vae_best.pt'
    data_path = '/kaggle/input/organized-gait-dataset/Normal_line'
    results_folder = './results'
    latents_cache_folder = './latents_cache'  # é¢„å¤„ç†ç¼“å­˜
    
    # === æ•°æ®é…ç½® ===
    num_users = 31  # ID_1 åˆ° ID_31
    images_per_user_total = 150  # æ¯ç”¨æˆ·æ€»å›¾åƒæ•°
    images_per_user_train = 50   # æ¯ç”¨æˆ·ç”¨äºDDPMè®­ç»ƒçš„å›¾åƒæ•°
    # å‰©ä½™100å¼ ä½œä¸ºæµ‹è¯•é›†ï¼Œä»…ç”¨äºåˆ†ç±»å™¨è¯„ä¼°ï¼Œå¯¹DDPMä¸å¯è§
    image_size = 256
    latent_size = 32  # VAEæ˜¯8å€ä¸‹é‡‡æ ·ï¼ˆ256/8=32ï¼‰
    latent_channels = 4
    
    # === æ¨¡å‹é…ç½®ï¼ˆé’ˆå¯¹å¾®å¤šæ™®å‹’æ•°æ®ä¼˜åŒ–ï¼‰===
    # å…³é”®è€ƒè™‘ï¼šç”¨æˆ·é—´å·®å¼‚æå°ï¼Œéœ€è¦å¼ºå¤§çš„æ¡ä»¶ç¼–ç å’Œåˆ¤åˆ«èƒ½åŠ›
    # å®é™…å‚æ•°é‡çº¦44M
    dim = 96  # åŸºç¡€ç»´åº¦ï¼šå¹³è¡¡æ¨¡å‹å®¹é‡ä¸è¿‡æ‹Ÿåˆé£é™©
    dim_mults = (1, 2, 4, 4)  # 4å±‚ç»“æ„ï¼šé¿å…æœ€åä¸€å±‚è¿‡å¤§
    attn_dim_head = 64  # æ³¨æ„åŠ›å¤´ç»´åº¦ï¼šå¢åŠ åˆ°64ä»¥åŒ¹é…classes_dim=384ï¼ˆ64Ã—8=512>384ï¼‰
    attn_heads = 8  # å¢åŠ æ³¨æ„åŠ›å¤´æ•°ï¼šæ›´å¥½æ•æ‰ç”¨æˆ·é—´å¾®å¦™å·®å¼‚
    cond_drop_prob = 0.0  # âŒ å…³é—­CFGè®­ç»ƒï¼ˆæå°æ•°æ®é›†åº”ä¸“æ³¨äºconditionalè´¨é‡ï¼‰
    # åˆ†æï¼š
    #   - æˆ‘ä»¬çš„ç›®æ ‡ï¼šæ¡ä»¶ç”Ÿæˆï¼ˆéœ€è¦å¼ºå¤§çš„conditionalåˆ†æ”¯ï¼‰
    #   - CFGè®­ç»ƒä¼šå‰Šå¼±conditionalï¼ˆ50â†’35å¼ /ç±»ï¼Œå‡å°‘30%ï¼‰
    #   - å¯¹äº50å¼ /ç±»çš„æå°æ•°æ®é›†ï¼Œä¸åº”è¯¥ä¸ºCFGç‰ºç‰²conditionalè®­ç»ƒ
    #   - ç»“è®ºï¼šæ”¾å¼ƒCFGï¼Œå…¨åŠ›è®­ç»ƒconditional
    
    # === æ‰©æ•£é…ç½® ===
    timesteps = 1000
    sampling_timesteps = 150  # DDIMé‡‡æ ·æ­¥æ•°ï¼ˆ100æ­¥è¶³å¤Ÿï¼Œè´¨é‡å¥½ä¸”å¿«ï¼‰
    objective = 'pred_v'  # v-prediction
    beta_schedule = 'cosine'
    # === é‡‡æ ·é…ç½® ===
    # cond_drop=0.0æ—¶ï¼Œä¸ç”¨CFG
    cond_scale = 1.0  # ä¸ç”¨CFGï¼ˆæœªè®­ç»ƒunconditionalï¼‰
    rescaled_phi = 0.0  # ä¸ç”¨CFG++
    
    # === è®­ç»ƒé…ç½®ï¼ˆé’ˆå¯¹RTX 5880 48GB + å°æ•°æ®é›†ä¼˜åŒ–ï¼‰===
    train_batch_size = 12  # batch sizeï¼šæ¨¡å‹44Må‚æ•°ï¼Œ48GBæ˜¾å­˜ç»°ç»°æœ‰ä½™
    gradient_accumulate_every = 3  # æ¢¯åº¦ç´¯ç§¯ï¼šæœ‰æ•ˆbatch=36ï¼Œä¿æŒè®­ç»ƒç¨³å®šæ€§
    train_lr = 4e-5  # å­¦ä¹ ç‡ï¼šæ ‡å‡†å€¼
    train_num_steps = 60000  # è®­ç»ƒæ­¥æ•°ï¼šçº¦1290 epochs
    
    # Learning Rate Schedule
    use_lr_warmup = True  # ä½¿ç”¨å­¦ä¹ ç‡warmup
    warmup_steps = 1000  # å‰1000æ­¥warmup
    
    # === ä¼˜åŒ–é…ç½®ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆ + å¢åŠ å¤šæ ·æ€§ï¼‰===
    use_ema = True  # æ˜¯å¦ä½¿ç”¨EMAï¼ˆBaselineè®¾ä¸ºFalseï¼‰
    ema_decay = 0.995  # EMAå¹³æ»‘ï¼šå°æ•°æ®é›†é™ä½decayï¼Œå¢åŠ å¤šæ ·æ€§ï¼ˆ0.999â†’0.995ï¼‰
    ema_update_every = 10  # EMAæ›´æ–°é¢‘ç‡
    max_grad_norm = 1.0  # æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    adam_betas = (0.9, 0.99)  # Adamä¼˜åŒ–å™¨å‚æ•°
    weight_decay = 1e-4  # æƒé‡è¡°å‡ï¼šL2æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    
    # === Min-SNRä¼˜åŒ–ï¼ˆå°æ•°æ®é›†å…³é”®ï¼‰===
    # Min-SNRå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°å­¦ä¹ æ‰€æœ‰æ—¶é—´æ­¥ï¼Œé¿å…è¿‡åº¦å…³æ³¨ç®€å•æ ·æœ¬
    min_snr_loss_weight = True
    min_snr_gamma = 5  # gamma=5é€‚åˆå°æ•°æ®é›†
    
    # === å¯¹æ¯”å­¦ä¹ é…ç½® ===
    # âš ï¸ å…³é”®å‘ç°ï¼šå¯¹æ¯”å­¦ä¹ å¯¼è‡´æ¨¡å¼å´©æºƒï¼ˆæ‰€æœ‰ç”¨æˆ·ç”Ÿæˆç›¸ä¼¼å›¾åƒï¼‰
    use_contrastive_loss = False  # âŒ å…³é—­å¯¹æ¯”å­¦ä¹ ï¼ˆå®éªŒè¯æ˜ï¼šé™ä½ç”¨æˆ·ç‰¹è‰²ï¼‰
    contrastive_weight = 0.0  # æƒé‡è®¾ä¸º0
    # é—®é¢˜è¯Šæ–­ï¼š
    #   - å¯¹æ¯”å­¦ä¹ è¿‡åº¦ä¼˜åŒ–"åŒç±»ç›¸ä¼¼"ç›®æ ‡
    #   - å¯¼è‡´ç”Ÿæˆ"å¹³å‡æ¨¡å¼"è€ŒéçœŸå®ç”¨æˆ·ç‰¹å¾
    #   - Baselineï¼ˆæ— å¯¹æ¯”å­¦ä¹ ï¼‰åè€Œä¿ç•™æ›´å¤šç”¨æˆ·ç‰¹è‰²
    
    contrastive_temperature = 0.07  # SupConæ¸©åº¦å‚æ•°ï¼ˆæ ‡å‡†å€¼ï¼‰
    
    contrastive_start_step = 5000  # å»¶è¿Ÿå¯åŠ¨ï¼ˆ2000â†’5000ï¼Œè®©æ‰©æ•£å…ˆç¨³å®šï¼‰
    # è°ƒæ•´ç†ç”±ï¼š
    #   - å°æ•°æ®é›†éœ€è¦æ‰©æ•£æ¨¡å‹å…ˆå­¦ä¼šåŸºæœ¬ç”Ÿæˆèƒ½åŠ›
    #   - è¿‡æ—©åŠ å…¥å¯¹æ¯”å­¦ä¹ å¯èƒ½å¹²æ‰°æ‰©æ•£è®­ç»ƒ
    #   - 5000æ­¥â‰ˆ107 epochsï¼Œæ‰©æ•£æ¨¡å‹å·²åŸºæœ¬ç¨³å®š
    
    # === å½’ä¸€åŒ–é…ç½® ===
    # âš ï¸ é‡è¦ï¼šè¿è¡Œ test_vae_range.py ç¡®å®šæ­¤å‚æ•°ï¼
    # å¦‚æœVAEæ½œåœ¨è¡¨ç¤ºåœ¨[0,1]èŒƒå›´ â†’ auto_normalize=True
    # å¦‚æœä¸åœ¨[0,1]èŒƒå›´ â†’ auto_normalize=False
    auto_normalize = False  # é»˜è®¤Falseï¼Œè¿è¡Œtest_vae_range.pyåæ ¹æ®ç»“æœè°ƒæ•´
    
    # === ç›‘æ§é…ç½® ===
    save_and_sample_every = 1000  # æ¯1000æ­¥ä¿å­˜ï¼ˆå°æ•°æ®é›†æ›´é¢‘ç¹è§‚å¯Ÿï¼Œé¿å…é”™è¿‡æœ€ä½³ç‚¹ï¼‰
    num_samples = 16  # ç”Ÿæˆ16å¼ æ£€æŸ¥
    
    # === å…¶ä»– ===
    amp = False  # æ··åˆç²¾åº¦ï¼ˆP100ä¸æ”¯æŒTensor Coresï¼ŒFP16åè€Œä¸ç¨³å®šï¼‰
    num_workers = 0  # Windowsä¸Šè®¾ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜ï¼Œæ•°æ®å·²ç¼“å­˜æ‰€ä»¥å½±å“ä¸å¤§
    seed = 42
    
    def print_config_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("\n" + "="*60)
        print("è®­ç»ƒé…ç½®æ‘˜è¦")
        print("="*60)
        
        print(f"\næ•°æ®: {self.num_users}ç”¨æˆ· Ã— {self.images_per_user_train}å¼  = {self.num_users * self.images_per_user_train}å¼ ")
        
        print(f"\næ¨¡å‹: dim={self.dim}, å±‚æ•°={len(self.dim_mults)}, æ³¨æ„åŠ›={self.attn_heads}å¤´")
        print(f"      ä¼°ç®—å‚æ•°é‡ ~{self._estimate_params():.1f}Mï¼ˆå®é™…ä»¥è®­ç»ƒæ—¶æ˜¾ç¤ºä¸ºå‡†ï¼‰")
        
        print(f"\nCFG: drop={self.cond_drop_prob}, scale={self.cond_scale}")
        
        print(f"\nè®­ç»ƒ: batch={self.train_batch_size}Ã—{self.gradient_accumulate_every}, lr={self.train_lr}, steps={self.train_num_steps:,}")
        
        # LR Warmup
        if self.use_lr_warmup:
            print(f"\nLR Warmup: {self.warmup_steps}æ­¥")
        
        # å¯¹æ¯”å­¦ä¹ é…ç½®
        if self.use_contrastive_loss:
            print(f"\nå¯¹æ¯”å­¦ä¹ : å¯ç”¨")
            print(f"      æƒé‡={self.contrastive_weight}, æ¸©åº¦={self.contrastive_temperature}")
            print(f"      å¼€å§‹æ­¥æ•°={self.contrastive_start_step}")
        else:
            print(f"\nå¯¹æ¯”å­¦ä¹ : æœªå¯ç”¨")
        
        print("="*60 + "\n")
    
    def _estimate_params(self):
        """ä¼°è®¡æ¨¡å‹å‚æ•°é‡ï¼ˆç™¾ä¸‡ï¼‰"""
        # æ›´å‡†ç¡®çš„UNetå‚æ•°ä¼°è®¡
        total = 0
        
        # åˆå§‹å·ç§¯
        total += self.dim * self.latent_channels * 7 * 7
        
        # Encoder/Decoderå±‚ï¼ˆæ¯å±‚2ä¸ªResBlock + æ³¨æ„åŠ›ï¼‰
        dims = [self.dim * m for m in self.dim_mults]
        for i in range(len(dims)):
            d = dims[i]
            # ResBlockå‚æ•°ï¼ˆ2ä¸ªå·ç§¯ + æ—¶é—´/ç±»åˆ«åµŒå…¥ï¼‰
            total += d * d * 3 * 3 * 4  # 4ä¸ªResBlock
            # æ³¨æ„åŠ›å‚æ•°
            total += d * d * 4  # QKV + output projection
        
        # æ—¶é—´åµŒå…¥MLP
        time_dim = self.dim * 4
        total += self.dim * time_dim + time_dim * time_dim
        
        # ç±»åˆ«åµŒå…¥MLP
        total += self.dim * time_dim + time_dim * time_dim
        
        # ä¸­é—´å±‚
        total += dims[-1] * dims[-1] * 3 * 3 * 2
        
        return total / 1e6


# ============================================================
# æ•°æ®é›†ç±»
# ============================================================
class LatentDataset(Dataset):
    """
    åŠ è½½é¢„å¤„ç†çš„æ½œåœ¨è¡¨ç¤º
    å¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼Œä¼šè‡ªåŠ¨ä»åŸå§‹å›¾åƒç¼–ç 
    
    æ•°æ®åˆ’åˆ†ï¼š
    - æ¯ç”¨æˆ·150å¼ å›¾åƒï¼Œä»…ä½¿ç”¨å‰50å¼ è®­ç»ƒDDPM
    - å100å¼ ä¿ç•™ä½œä¸ºæµ‹è¯•é›†ï¼Œç”¨äºåˆ†ç±»å™¨è¯„ä¼°
    """
    def __init__(self, vae, data_path, latents_cache_folder, 
                 num_users=31, images_per_user=50, seed=42):
        super().__init__()
        self.vae = vae
        self.data_path = Path(data_path)
        self.latents_cache_folder = Path(latents_cache_folder)
        self.seed = seed
        
        # æ”¶é›†æ‰€æœ‰å›¾åƒè·¯å¾„å’Œæ ‡ç­¾
        self.samples = []
        
        # å°è¯•ä»data_split.jsonåŠ è½½é¢„å¤„ç†çš„åˆ’åˆ†
        split_file = self.latents_cache_folder / 'data_split.json'
        use_precomputed_split = False
        
        if split_file.exists():
            import json
            try:
                with open(split_file, 'r', encoding='utf-8') as f:
                    split_info = json.load(f)
                
                sampling_method = split_info.get('sampling_method', 'unknown')
                print(f"âœ“ æ‰¾åˆ°é¢„å¤„ç†çš„æ•°æ®åˆ’åˆ†: {split_file}")
                print(f"  é‡‡æ ·æ–¹æ³•: {sampling_method}")
                
                # ä½¿ç”¨é¢„å¤„ç†çš„è®­ç»ƒé›†åˆ’åˆ†
                for user_key, user_info in split_info['users'].items():
                    user_id = user_info['user_id']
                    label = user_info['label']
                    
                    # è·å–è®­ç»ƒé›†æ–‡ä»¶è·¯å¾„
                    for rel_path in user_info['train_images']:
                        img_path = self.data_path / rel_path
                        if img_path.exists():
                            self.samples.append((img_path, label))
                
                use_precomputed_split = True
                print(f"âœ“ ä½¿ç”¨é¢„å¤„ç†çš„è®­ç»ƒé›†åˆ’åˆ† ({sampling_method})")
                
            except Exception as e:
                print(f"Warning: æ— æ³•åŠ è½½data_split.json: {e}")
                print("  å°†ä½¿ç”¨éšæœºæŠ½æ ·")
        
        # å¦‚æœæ²¡æœ‰é¢„å¤„ç†çš„åˆ’åˆ†ï¼Œä½¿ç”¨éšæœºæŠ½æ ·ï¼ˆæ—§æ–¹æ³•ï¼‰
        if not use_precomputed_split:
            print("æœªæ‰¾åˆ°é¢„å¤„ç†çš„æ•°æ®åˆ’åˆ†ï¼Œä½¿ç”¨éšæœºæŠ½æ ·")
            
            for user_id in range(1, num_users + 1):
                user_folder = self.data_path / f"ID_{user_id}"
                if not user_folder.exists():
                    print(f"Warning: {user_folder} not found, skipping...")
                    continue
                
                # æ”¶é›†è¯¥ç”¨æˆ·çš„æ‰€æœ‰jpgå›¾åƒï¼ˆæ’åºç¡®ä¿ä¸€è‡´æ€§ï¼‰
                image_paths = sorted(list(user_folder.glob("*.jpg")))
                
                # ä¸ºæ¯ä¸ªç”¨æˆ·è®¾ç½®ç‹¬ç«‹ä½†å¯å¤ç°çš„éšæœºç§å­
                user_seed = seed + user_id
                rng = np.random.RandomState(user_seed)
                
                # éšæœºæ‰“ä¹±ï¼ˆå›ºå®šç§å­ï¼‰
                indices = rng.permutation(len(image_paths))
                image_paths = [image_paths[i] for i in indices]
                
                # ä»…ä½¿ç”¨å‰images_per_userå¼ 
                train_paths = image_paths[:images_per_user]
                
                for img_path in train_paths:
                    self.samples.append((img_path, user_id - 1))  # label: 0-30
        
        print(f"DDPMè®­ç»ƒé›†: {len(self.samples)} å¼ å›¾åƒ ({num_users}ç”¨æˆ· Ã— {images_per_user}å¼ /ç”¨æˆ·)")
        
        # å›¾åƒé¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(256),
            transforms.ToTensor()  # è‡ªåŠ¨å½’ä¸€åŒ–åˆ°[0,1]
        ])
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        # ä½¿ç”¨ userID_filename.pt æ ¼å¼é¿å…æ–‡ä»¶åå†²çª
        cache_filename = f"user_{label:02d}_{img_path.stem}.pt"
        cache_path = self.latents_cache_folder / cache_filename
        
        if cache_path.exists():
            latent = torch.load(cache_path, map_location='cpu', weights_only=True)
        else:
            # ä»åŸå§‹å›¾åƒç¼–ç 
            img = Image.open(img_path).convert('RGB')
            img = self.transform(img)
            
            with torch.no_grad():
                img_batch = img.unsqueeze(0).to(next(self.vae.parameters()).device)
                latent = self.vae.encode_images(img_batch)  # [1, 4, 32, 32]
                latent = latent.squeeze(0).cpu()  # [4, 32, 32]
            
            # ä¿å­˜ç¼“å­˜
            cache_path.parent.mkdir(parents=True, exist_ok=True)
            torch.save(latent, cache_path)
        
        return latent, label


# ============================================================
# è®­ç»ƒå™¨
# ============================================================
class LatentDiffusionTrainer:
    """ç®€åŒ–çš„è®­ç»ƒå™¨ï¼Œé›†æˆAccelerator"""
    
    def __init__(self, config):
        self.config = config
        
        # æ‰“å°é…ç½®æ‘˜è¦
        config.print_config_summary()
        
        # åˆå§‹åŒ–Accelerator
        self.accelerator = Accelerator(
            split_batches=True,
            mixed_precision='fp16' if config.amp else 'no'
        )
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(config.seed)
        np.random.seed(config.seed)
        
        # åŠ è½½VAE
        print("Loading VAE...")
        print(f"  VAEæ¨¡å—è·¯å¾„: {KL_VAE.__module__}")
        import vae.kl_vae
        print(f"  VAEæ–‡ä»¶è·¯å¾„: {vae.kl_vae.__file__}")
        
        # åŠ è½½checkpoint
        checkpoint = torch.load(config.vae_path, map_location='cpu')
        
        # æ£€æŸ¥checkpointæ ¼å¼
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            # è®­ç»ƒæ£€æŸ¥ç‚¹æ ¼å¼
            print("  Detected training checkpoint format")
            vae_state = checkpoint['model_state_dict']
            
            # è·å–é…ç½®ä¿¡æ¯
            embed_dim = checkpoint.get('embed_dim', 4)
            scale_factor = checkpoint.get('scale_factor', 0.18215)
            print(f"  embed_dim: {embed_dim}, scale_factor: {scale_factor}")
        else:
            # ç›´æ¥state_dictæ ¼å¼
            print("  Detected direct state_dict format")
            vae_state = checkpoint
            embed_dim = 4
            scale_factor = 0.18215
        
        # åˆ›å»ºVAEå¹¶åŠ è½½æƒé‡
        self.vae = KL_VAE(embed_dim=embed_dim, scale_factor=scale_factor)
        self.vae.load_state_dict(vae_state)
        self.vae.eval()
        self.vae.requires_grad_(False)
        self.vae = self.vae.to(self.accelerator.device)
        print(f"VAE loaded from {config.vae_path}")
        
        # åˆ›å»ºæ•°æ®é›†
        print("Creating dataset...")
        train_ds = LatentDataset(
            self.vae, config.data_path, config.latents_cache_folder,
            config.num_users, 
            images_per_user=config.images_per_user_train,
            seed=config.seed
        )
        
        # åˆ›å»ºDataLoader
        self.train_dl = DataLoader(
            train_ds, 
            batch_size=config.train_batch_size,
            shuffle=True,
            num_workers=config.num_workers,
            pin_memory=True
        )
        
        # åˆ›å»ºUnetæ¨¡å‹
        print("Creating Unet model...")
        self.model = Unet(
            dim=config.dim,
            dim_mults=config.dim_mults,
            num_classes=config.num_users,
            cond_drop_prob=config.cond_drop_prob,
            channels=config.latent_channels,
            attn_dim_head=config.attn_dim_head,
            attn_heads=config.attn_heads,
            learned_variance=False
        )
        
        # è®¡ç®—å‚æ•°é‡
        num_params = sum(p.numel() for p in self.model.parameters())
        print(f"Model parameters: {num_params/1e6:.2f}M")
        
        # åˆ›å»ºæ‰©æ•£æ¨¡å‹
        print("Creating diffusion model...")
        self.diffusion = GaussianDiffusion(
            self.model,
            image_size=config.latent_size,
            timesteps=config.timesteps,
            sampling_timesteps=config.sampling_timesteps,
            objective=config.objective,
            beta_schedule=config.beta_schedule,
            min_snr_loss_weight=config.min_snr_loss_weight,
            min_snr_gamma=config.min_snr_gamma,
            auto_normalize=config.auto_normalize  # ä»Configè¯»å–
        )
        
        print(f"Auto normalize: {config.auto_normalize}")
        if not config.auto_normalize:
            print("  â†’ æ½œåœ¨ç©ºé—´ä¸åš[0,1]â†’[-1,1]å½’ä¸€åŒ–")
        else:
            print("  â†’ æ½œåœ¨ç©ºé—´ä¼šè¢«å½’ä¸€åŒ–åˆ°[-1,1]")
        
        # åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆæ·»åŠ weight decayé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        self.opt = torch.optim.Adam(
            self.diffusion.parameters(),
            lr=config.train_lr,
            betas=config.adam_betas,
            weight_decay=config.weight_decay
        )
        
        # å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if config.use_contrastive_loss:
            print(f"\nâœ“ å¯ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±")
            print(f"  æƒé‡: {config.contrastive_weight}")
            print(f"  æ¸©åº¦: {config.contrastive_temperature}")
            print(f"  å¼€å§‹æ­¥æ•°: {config.contrastive_start_step}")
            
            from losses import SupConLoss
            self.contrastive_criterion = SupConLoss(
                temperature=config.contrastive_temperature,
                contrast_mode='all',
                base_temperature=config.contrastive_temperature
            )
        else:
            print("\nâœ— æœªå¯ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±")
            self.contrastive_criterion = None
        
        # ä½¿ç”¨Acceleratorå‡†å¤‡
        self.diffusion, self.opt, self.train_dl = self.accelerator.prepare(
            self.diffusion, self.opt, self.train_dl
        )
        
        # EMAï¼ˆä»…ä¸»è¿›ç¨‹ï¼Œå¯é€‰ï¼‰
        self.use_ema = config.use_ema
        if self.use_ema and self.accelerator.is_main_process:
            self.ema = EMA(
                self.diffusion,
                beta=config.ema_decay,
                update_every=config.ema_update_every
            )
            self.ema.to(self.accelerator.device)
        else:
            self.ema = None
        
        # åˆ›å»ºç»“æœæ–‡ä»¶å¤¹
        self.results_folder = Path(config.results_folder)
        if self.accelerator.is_main_process:
            self.results_folder.mkdir(exist_ok=True, parents=True)
        
        self.step = 0
        
        # å¼‚å¸¸æ£€æµ‹
        self.loss_history = []
        self.nan_count = 0
        self.high_loss_count = 0
        
        # å¯¹æ¯”å­¦ä¹ æŸå¤±å†å²ï¼ˆç”¨äºç›‘æ§ï¼‰
        self.contrastive_loss_history = []
    
    def train(self):
        """è®­ç»ƒå¾ªç¯"""
        config = self.config
        
        # åˆ›å»ºæ— é™å¾ªç¯çš„dataloader
        def cycle(dl):
            while True:
                for data in dl:
                    yield data
        
        dl = cycle(self.train_dl)
        
        with tqdm(
            initial=self.step,
            total=config.train_num_steps,
            disable=not self.accelerator.is_main_process
        ) as pbar:
            while self.step < config.train_num_steps:
                self.diffusion.train()
                total_loss = 0.
                total_diffusion_loss = 0.
                total_contrastive_loss = 0.
                
                # æ¢¯åº¦ç´¯ç§¯
                for grad_accum_idx in range(config.gradient_accumulate_every):
                    latents, labels = next(dl)
                    latents = latents.to(self.accelerator.device)
                    labels = labels.to(self.accelerator.device)
                    
                    # è°ƒè¯•ï¼šç¬¬ä¸€æ­¥æ‰“å°æ•°æ®èŒƒå›´
                    if self.step == 0 and grad_accum_idx == 0 and self.accelerator.is_main_process:
                        print(f"\n[è°ƒè¯•] æ•°æ®èŒƒå›´æ£€æŸ¥:")
                        print(f"  latents shape: {latents.shape}")
                        print(f"  latents min: {latents.min().item():.4f}")
                        print(f"  latents max: {latents.max().item():.4f}")
                        print(f"  latents mean: {latents.mean().item():.4f}")
                        print(f"  latents std: {latents.std().item():.4f}")
                    
                    with self.accelerator.autocast():
                        # è®¡ç®—æ‰©æ•£æŸå¤±ï¼ˆå¦‚æœå¯ç”¨å¯¹æ¯”å­¦ä¹ ï¼ŒåŒæ—¶æå–ç‰¹å¾ï¼‰
                        if (config.use_contrastive_loss and 
                            self.contrastive_criterion is not None and
                            self.step >= config.contrastive_start_step):
                            
                            # ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼ŒåŒæ—¶è®¡ç®—æŸå¤±å’Œæå–ç‰¹å¾
                            # éœ€è¦è®¿é—®åº•å±‚æ¨¡å‹ï¼ˆå»é™¤AcceleratoråŒ…è£…ï¼‰
                            unwrapped_diffusion = self.accelerator.unwrap_model(self.diffusion)
                            
                            # éšæœºæ—¶é—´æ­¥
                            b = latents.shape[0]
                            device = latents.device
                            t = torch.randint(0, unwrapped_diffusion.num_timesteps, (b,), device=device).long()
                            
                            # å½’ä¸€åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
                            img = unwrapped_diffusion.normalize(latents)
                            
                            # è°ƒç”¨p_lossesï¼ŒåŒæ—¶è¿”å›ç‰¹å¾
                            diffusion_loss, features = unwrapped_diffusion.p_losses(
                                img, t, 
                                classes=labels, 
                                return_features=True
                            )
                            
                            # å‡†å¤‡SupConè¾“å…¥æ ¼å¼: [B, n_views, C]
                            features = features.unsqueeze(1)  # [B, 1, C]
                            
                            # è®¡ç®—SupConæŸå¤±
                            contrastive_loss = self.contrastive_criterion(features, labels)
                            contrastive_loss = contrastive_loss * config.contrastive_weight
                        else:
                            # æ ‡å‡†æ‰©æ•£æŸå¤±ï¼ˆä¸æå–ç‰¹å¾ï¼‰
                            diffusion_loss = self.diffusion(latents, classes=labels)
                            contrastive_loss = 0.
                        
                        # æ··åˆæŸå¤±
                        loss = diffusion_loss + contrastive_loss
                        
                        # è®°å½•å„é¡¹æŸå¤±ï¼ˆåœ¨é™¤ä»¥gradient_accumulate_everyä¹‹å‰ï¼‰
                        total_loss += loss.item()
                        total_diffusion_loss += diffusion_loss.item()
                        if isinstance(contrastive_loss, torch.Tensor):
                            total_contrastive_loss += contrastive_loss.item()
                        
                        # ä¸ºæ¢¯åº¦ç´¯ç§¯ç¼©æ”¾æŸå¤±
                        loss = loss / config.gradient_accumulate_every
                    
                    self.accelerator.backward(loss)
                
                # æ¢¯åº¦è£å‰ª
                self.accelerator.clip_grad_norm_(
                    self.diffusion.parameters(),
                    config.max_grad_norm
                )
                
                # ä¼˜åŒ–å™¨æ­¥è¿›
                self.opt.step()
                self.opt.zero_grad()
                
                # Learning Rate Warmupï¼ˆå°æ•°æ®é›†ç¨³å®šè®­ç»ƒï¼‰
                if config.use_lr_warmup and self.step < config.warmup_steps:
                    # çº¿æ€§warmup
                    lr_scale = (self.step + 1) / config.warmup_steps
                    for param_group in self.opt.param_groups:
                        param_group['lr'] = config.train_lr * lr_scale
                
                self.accelerator.wait_for_everyone()
                
                # æ›´æ–°è¿›åº¦ï¼ˆæ˜¾ç¤ºè¯¦ç»†æŸå¤±ï¼‰
                if config.use_contrastive_loss and self.step >= config.contrastive_start_step:
                    pbar.set_description(
                        f'loss: {total_loss:.4f} | '
                        f'diff: {total_diffusion_loss:.4f} | '
                        f'contr: {total_contrastive_loss:.4f}'
                    )
                else:
                    pbar.set_description(f'loss: {total_loss:.4f}')
                
                self.step += 1
                
                # è®°å½•å¯¹æ¯”å­¦ä¹ æŸå¤±
                if config.use_contrastive_loss and total_contrastive_loss > 0:
                    self.contrastive_loss_history.append(total_contrastive_loss)
                
                # å¼‚å¸¸æ£€æµ‹
                if self.accelerator.is_main_process:
                    self._check_training_health(total_loss)
                
                # EMAæ›´æ–°
                if self.ema and self.accelerator.is_main_process:
                    self.ema.update()
                
                # å®šæœŸä¿å­˜å’Œé‡‡æ ·ï¼ˆä¸ä¾èµ–EMAï¼‰
                if self.accelerator.is_main_process:
                    if self.step % config.save_and_sample_every == 0:
                        self.save_and_sample(self.step // config.save_and_sample_every)
                
                pbar.update(1)
        
        print('Training complete!')
    
    def _check_training_health(self, loss):
        """æ£€æµ‹è®­ç»ƒå¼‚å¸¸"""
        import math
        
        # è®°å½•losså†å²ï¼ˆæœ€è¿‘100æ­¥ï¼‰
        self.loss_history.append(loss)
        if len(self.loss_history) > 100:
            self.loss_history.pop(0)
        
        # æ£€æŸ¥1: NaNæˆ–Inf
        if math.isnan(loss) or math.isinf(loss):
            self.nan_count += 1
            print(f"\nâš ï¸ è­¦å‘Š: Loss is NaN/Inf (ç¬¬{self.nan_count}æ¬¡)")
            
            if self.nan_count >= 3:
                print("\nâŒ ä¸¥é‡é”™è¯¯: LossæŒç»­NaN/Infï¼Œè®­ç»ƒå¤±è´¥ï¼")
                print("   å¯èƒ½åŸå› ï¼šå­¦ä¹ ç‡è¿‡å¤§ã€æ¢¯åº¦çˆ†ç‚¸ã€æ•°æ®é—®é¢˜")
                print("   å»ºè®®ï¼šé™ä½å­¦ä¹ ç‡æˆ–æ£€æŸ¥æ•°æ®")
                raise RuntimeError("Training diverged - Loss is NaN/Inf")
        
        # æ£€æŸ¥2: Losså¼‚å¸¸é«˜
        if loss > 1.0 and self.step > 1000:
            self.high_loss_count += 1
            if self.high_loss_count > 50:
                print(f"\nâš ï¸ è­¦å‘Š: LossæŒç»­å¼‚å¸¸é«˜ (>{loss:.4f})ï¼Œè®­ç»ƒå¯èƒ½æ— æ•ˆ")
                print("   æ£€æŸ¥ï¼šæ•°æ®æ˜¯å¦æ­£ç¡®åŠ è½½ï¼ŸVAEæ˜¯å¦æ­£ç¡®ï¼Ÿ")
        else:
            self.high_loss_count = 0  # é‡ç½®
        
        # æ£€æŸ¥3: Lossä¸ä¸‹é™ï¼ˆæ¯5000æ­¥æ£€æŸ¥ï¼‰
        if self.step % 5000 == 0 and self.step > 5000:
            if len(self.loss_history) >= 50:
                recent_avg = sum(self.loss_history[-50:]) / 50
                early_avg = sum(self.loss_history[:50]) / 50
                
                if recent_avg >= early_avg * 0.95:  # å‡ ä¹æ²¡ä¸‹é™
                    print(f"\nâš ï¸ æ³¨æ„: Lossä¸‹é™ç¼“æ…¢ ({early_avg:.4f} â†’ {recent_avg:.4f})")
                    print("   å¯èƒ½åŸå› ï¼šå­¦ä¹ ç‡è¿‡å°ã€å·²æ”¶æ•›ã€æˆ–è®­ç»ƒé—®é¢˜")
        
        # æ£€æŸ¥4: Lossè¿‡ä½ï¼ˆå¯èƒ½è¿‡æ‹Ÿåˆï¼‰
        if self.step > 50000 and loss < 0.0001:
            print(f"\nâš ï¸ è­¦å‘Š: Losséå¸¸ä½ ({loss:.6f})ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ/è®°å¿†è®­ç»ƒé›†")
            print("   å»ºè®®ï¼šæ£€æŸ¥ç”Ÿæˆæ ·æœ¬æ˜¯å¦ä¸è®­ç»ƒé›†å®Œå…¨ç›¸åŒ")
    
    def save_and_sample(self, milestone):
        """ä¿å­˜æ£€æŸ¥ç‚¹å¹¶ç”Ÿæˆæ ·æœ¬"""
        # ä½¿ç”¨EMAæ¨¡å‹æˆ–åŸå§‹æ¨¡å‹é‡‡æ ·
        model_for_sample = self.ema.ema_model if self.ema else self.diffusion
        model_for_sample.eval()
        
        print(f"\n{'='*60}")
        print(f"Checkpoint {milestone} (æ­¥æ•°: {self.step})")
        print(f"{'='*60}")
        
        # ç”Ÿæˆæ ·æœ¬ï¼ˆæ¯ä¸ªç”¨æˆ·1å¼ ï¼Œå…±num_sampleså¼ ï¼‰
        try:
            with torch.no_grad():
                # é€‰æ‹©è¦ç”Ÿæˆçš„ç”¨æˆ·
                num_samples = min(self.config.num_samples, self.config.num_users)
                user_ids = torch.arange(num_samples, device=self.accelerator.device)
                
                # DDPMé‡‡æ ·ï¼ˆæ¡ä»¶æ‰©æ•£ + DDIM + CFGï¼‰
                sampled_latents = model_for_sample.sample(
                    classes=user_ids,              # æ¡ä»¶ï¼šç”¨æˆ·ID
                    cond_scale=self.config.cond_scale,  # CFGå¼ºåº¦
                    rescaled_phi=self.config.rescaled_phi  # CFG++ rescaling
                )
                # é‡‡æ ·æ–¹å¼ï¼šDDIM 100æ­¥ï¼ˆæ¯”1000æ­¥å¿«10å€ï¼Œè´¨é‡ç›¸è¿‘ï¼‰
                
                # VAEè§£ç 
                sampled_images = self.vae.decode_latents(sampled_latents)
                
                # ä¿å­˜å›¾åƒç½‘æ ¼
                save_path = self.results_folder / f'sample-{milestone}.png'
                utils.save_image(
                    sampled_images,
                    str(save_path),
                    nrow=int(math.sqrt(num_samples))
                )
                print(f"âœ“ æ ·æœ¬å·²ä¿å­˜: {save_path}")
                
                # ç®€å•è´¨é‡æ£€æŸ¥
                img_min = sampled_images.min().item()
                img_max = sampled_images.max().item()
                img_mean = sampled_images.mean().item()
                
                if img_min < -0.1 or img_max > 1.1:
                    print(f"  âš ï¸ è­¦å‘Š: å›¾åƒå€¼å¼‚å¸¸ [{img_min:.3f}, {img_max:.3f}]")
                
                print(f"  å›¾åƒç»Ÿè®¡: min={img_min:.3f}, max={img_max:.3f}, mean={img_mean:.3f}")
                
        except Exception as e:
            print(f"  âœ— ç”Ÿæˆæ ·æœ¬å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        try:
            data = {
                'step': self.step,
                'model': self.accelerator.get_state_dict(self.diffusion),
                'opt': self.opt.state_dict(),
                'config': self.config.__dict__,
                'loss_history': self.loss_history[-100:],  # ä¿å­˜æœ€è¿‘100æ­¥loss
                'contrastive_loss_history': self.contrastive_loss_history[-100:]  # ä¿å­˜å¯¹æ¯”å­¦ä¹ æŸå¤±
            }
            if self.ema:
                data['ema'] = self.ema.state_dict()
            save_path = self.results_folder / f'model-{milestone}.pt'
            torch.save(data, str(save_path))
            print(f"âœ“ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {save_path}")
            
            # æ‰“å°å¯¹æ¯”å­¦ä¹ æŸå¤±ç»Ÿè®¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config.use_contrastive_loss and len(self.contrastive_loss_history) > 0:
                recent_contr_loss = sum(self.contrastive_loss_history[-100:]) / len(self.contrastive_loss_history[-100:])
                print(f"  å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆæœ€è¿‘100æ­¥å¹³å‡ï¼‰: {recent_contr_loss:.4f}")
            
            # åŒæ—¶ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹ï¼ˆè¦†ç›–ï¼‰
            latest_path = self.results_folder / 'model-latest.pt'
            torch.save(data, str(latest_path))
            print(f"âœ“ æœ€æ–°æ£€æŸ¥ç‚¹: {latest_path}")
            
            # ä¿å­˜æ‰€æœ‰checkpointï¼ˆä¸åˆ é™¤ï¼‰
            print(f"  ğŸ’¾ ä¿ç•™æ‰€æœ‰checkpointä»¥ä¾¿é€‰æ‹©æœ€ä½³æ¨¡å‹")
            
        except Exception as e:
            print(f"  âœ— ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        print(f"{'='*60}\n")
    
    def load(self, milestone):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        load_path = self.results_folder / f'model-{milestone}.pt'
        data = torch.load(str(load_path), map_location=self.accelerator.device)
        
        model = self.accelerator.unwrap_model(self.diffusion)
        model.load_state_dict(data['model'])
        
        self.step = data['step']
        self.opt.load_state_dict(data['opt'])
        
        if self.ema and self.accelerator.is_main_process and 'ema' in data:
            self.ema.load_state_dict(data['ema'])
        
        print(f"Loaded checkpoint from {load_path}, step {self.step}")


# ============================================================
# ä¸»å‡½æ•°
# ============================================================
def main():
    parser = argparse.ArgumentParser(description='Train Latent Diffusion Model')
    parser.add_argument('--resume', type=int, default=None, help='Resume from milestone')
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = Config()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LatentDiffusionTrainer(config)
    
    # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.resume is not None:
        trainer.load(args.resume)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()


if __name__ == '__main__':
    main()

